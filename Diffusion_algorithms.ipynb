{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Algorithms\n",
    "\n",
    "This notebook is meant to explore the fundamental of diffusion models before actually training the diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import nibabel as nib\n",
    "import monai\n",
    "from monai.data import Dataset, DataLoader, CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandScaleCrop,\n",
    "    RandFlip,\n",
    "    RandRotate90,\n",
    "    RandRotate,\n",
    "    RandShiftIntensity,\n",
    "    ToTensor,\n",
    "    RandSpatialCrop,\n",
    "    LoadImage,\n",
    "    SqueezeDim,\n",
    "    RandRotate,\n",
    "    RandShiftIntensity,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"Create sinusoidal timestep embeddings.\n",
    "\n",
    "    Args:\n",
    "        timesteps (Tensor): a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
    "        dim (int): the dimension of the output.\n",
    "        max_period (int, optional): controls the minimum frequency of the embeddings. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `t` timestep embeddings.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, t)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)\n",
    "\n",
    "\n",
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # pojection for time step embedding\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        `x` has shape `[batch_size, in_dim, height, width]`\n",
    "        `t` has shape `[batch_size, time_dim]`\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        # Add time step embeddings\n",
    "        t_emb = self.time_emb(t)\n",
    "        h += t_emb[:, :, None, None, None]\n",
    "        h = self.conv2(h)\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        \"\"\"\n",
    "        Attention block with shortcut\n",
    "\n",
    "        Args:\n",
    "            channels (int): channels\n",
    "            num_heads (int, optional): attention heads. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "\n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv3d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv3d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W= x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B*self.num_heads, -1, D*H*W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, D, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv3d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv3d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool3d(stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full UNet model with attention and timestep embedding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        model_channels=128,\n",
    "        out_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=(8, 16),\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 2, 2),\n",
    "        conv_resample=True,\n",
    "        num_heads=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # time embedding\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv3d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch, mult * model_channels, time_embed_dim, dropout)\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1: # don't use downsample for the last stage\n",
    "                self.down_blocks.append(TimestepEmbedSequential(Downsample(ch, conv_resample)))\n",
    "                down_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        # middle block\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout)\n",
    "        )\n",
    "\n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(\n",
    "                        ch + down_block_chans.pop(),\n",
    "                        model_channels * mult,\n",
    "                        time_embed_dim,\n",
    "                        dropout\n",
    "                    )\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    layers.append(Upsample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv3d(model_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [N x C x H x W]\n",
    "            timesteps (Tensor): a 1-D batch of timesteps.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: [N x C x ...]\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        # down stage\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        # middle stage\n",
    "        h = self.middle_block(h, emb)\n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            cat_in = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = module(cat_in, emb)\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start_scale=0.0001, end_scale=0.02):\n",
    "    \"\"\"\n",
    "    beta schedule\n",
    "    \"\"\"\n",
    "    ###############################################################\n",
    "    # to compute : linear beta schedule\n",
    "    # should be a linear beta between start_scale to end_scale\n",
    "    ###############################################################\n",
    "    return  torch.linspace(start_scale, end_scale, timesteps)\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    \"\"\"\n",
    "    ###############################################################\n",
    "    # to compute : cosine beta schedule\n",
    "    # should be a cosine beta between start_scale to end_scale\n",
    "    ###############################################################\n",
    "    t = torch.arange(0, timesteps+1)\n",
    "    f0 = np.cos(1/(1+s)*np.pi/2)**2\n",
    "    alpha_t = np.cos((t/t[-1]+1)/(1+s)*np.pi/2)**2 / f0\n",
    "    return 1 - (alpha_t / (alpha_t - 1))\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        beta_schedule='linear'\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "        self.betas = betas\n",
    "\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning\n",
    "        # of the diffusion chain\n",
    "        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n",
    "\n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * torch.sqrt(self.alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        # get the param of given timestep t\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        # forward diffusion (using the nice property): q(x_t | x_0)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        ###############################################################\n",
    "        # to compute : sqrt_alphas_cumprod_t\n",
    "        #              sqrt_one_minus_alphas_cumprod_t\n",
    "        #\n",
    "        # and finish the noise blending from x_start (i.e., x_0) to x_t\n",
    "        ###############################################################\n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return x_t\n",
    "        ###############################################################\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        # Get the mean and variance of q(x_t | x_0).\n",
    "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        # Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
    "        posterior_mean = (\n",
    "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        # compute x_0 from x_t and pred noise: the reverse of `q_sample`\n",
    "        ###############################################################\n",
    "        # from x_t to x_0,\n",
    "        ###############################################################\n",
    "        x_0 = (x_t - self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * noise) / self._extract(self.sqrt_alphas_cumprod, t, x_t.shape)\n",
    "        return x_0\n",
    "        ###############################################################\n",
    "\n",
    "    def p_mean_variance(self, model, x_t, t, clip_denoised=True):\n",
    "        # compute predicted mean and variance of p(x_{t-1} | x_t)\n",
    "        # predict noise using model\n",
    "        pred_noise = model(x_t, t)\n",
    "        # get the predicted x_0: different from the algorithm2 in the paper\n",
    "        ###############################################################\n",
    "        # model_mean, posterior_variance, posterior_log_variance\n",
    "        # the predicted mean, variance and log variance of p(x_{t-1} | x_t)\n",
    "        # do not forget to clip the denoised x_0 by\n",
    "        # torch.clamp(x_reconstructed, min=-1., max=1.)\n",
    "        # re-use q_posterior_mean_variance and predict_start_from_noise\n",
    "        ###############################################################\n",
    "        x_reconstructed = self.predict_start_from_noise(x_t, t, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x_reconstructed = torch.clamp(x_reconstructed, min=-1., max=1.)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior_mean_variance(x_reconstructed,x_t, t)\n",
    "        ###############################################################\n",
    "\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t, clip_denoised=True):\n",
    "        # denoise_step: sample x_{t-1} from x_t and pred_noise\n",
    "        # predict mean and variance\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t, clip_denoised=clip_denoised)\n",
    "        noise = torch.randn_like(x_t)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
    "        # compute x_{t-1}\n",
    "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        # denoise: reverse diffusion\n",
    "        batch_size = shape[0]\n",
    "        device = next(model.parameters()).device\n",
    "        ###############################################################\n",
    "        # start from pure noise (for each example in the batch)\n",
    "        # loop sample x_{t-1} from x_t\n",
    "        # return the list of sampled images\n",
    "        ###############################################################\n",
    "        imgs = []\n",
    "        x_t = torch.randn(*shape, device=device)\n",
    "        for t in range(self.timesteps - 1, -1, -1):\n",
    "            x_t = self.p_sample(model, x_t, torch.full((batch_size,), t, device=device))\n",
    "            imgs.append(x_t)\n",
    "        ###############################################################\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=1, channels=1):\n",
    "        # sample new images\n",
    "        return self.p_sample_loop(model, shape=(batch_size, channels, image_size, image_size, image_size))\n",
    "\n",
    "    def train_losses(self, model, x_start, t):\n",
    "        \"\"\" compute train losses \"\"\"\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "        noised_x = self.q_sample(x_start, t, noise)\n",
    "        predicted_noise = model(noised_x, t)\n",
    "        loss = F.mse_loss(predicted_noise, noise)        \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor(0.9721, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define model and diffusion\n",
    "timesteps = 500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNetModel(\n",
    "    in_channels=1,\n",
    "    model_channels=96,\n",
    "    out_channels=1,\n",
    "    channel_mult=(1, 2, 2),\n",
    "    attention_resolutions=[]\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=timesteps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "print(device)\n",
    "\n",
    "# test the untrained model on the diffusion process\n",
    "x = torch.zeros(1, 1, 16, 16, 16)\n",
    "for i in range(16):\n",
    "    x[0, :, i, i, i] = 1\n",
    "\n",
    "x = x.to(device)\n",
    "t = torch.randint(0, timesteps, (x.shape[0],), device=device)\n",
    "optimizer.zero_grad()\n",
    "loss = gaussian_diffusion.train_losses(model, x, t)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for T1w, T2w, and DWI files in data//data-multi-subject// ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 267 T1w files and 267 T2w files.\n"
     ]
    }
   ],
   "source": [
    "# this cell aims at extracting the list of path relevant for the first model test which takes T1w T2w adn DWI as image\n",
    "\n",
    "base_dir=\"data//data-multi-subject//\"\n",
    "\n",
    "desired_extension = \".json\"\n",
    "\n",
    "# Initialize lists to store the relative paths for T1w, T2w, and DWI files\n",
    "t1w_file_paths = []\n",
    "t2w_file_paths = []\n",
    "\n",
    "print(\"Searching for T1w, T2w, and DWI files in\", base_dir, \"...\")\n",
    "\n",
    "# Traverse the directory structure\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    # Exclude the \"derivatives\" subfolder\n",
    "    if \"derivatives\" in dirs:\n",
    "        dirs.remove(\"derivatives\")\n",
    "    for file in files:\n",
    "        # Check if the file name contains the desired names\n",
    "        if \"T1w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T1w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(base_dir + relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T1w file paths list\n",
    "            t1w_file_paths.append(relative_path)\n",
    "        elif \"T2w\" in file and file.endswith(desired_extension):\n",
    "            # Get the relative path of the T2w file\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), base_dir)\n",
    "            # Remove the file extension\n",
    "            relative_path = os.path.splitext(relative_path)[0] + \".nii.gz\"\n",
    "            # Append the relative path to the T2w file paths list\n",
    "            t2w_file_paths.append(base_dir + relative_path)\n",
    "\n",
    "#t1w_file_paths = t1w_file_paths[:20]\n",
    "#t2w_file_paths = t2w_file_paths[:20]\n",
    "\n",
    "print(\"Found\", len(t1w_file_paths), \"T1w files and\", len(t2w_file_paths), \"T2w files.\")\n",
    "\n",
    "# split the data into training and validation sets\n",
    "\n",
    "# build a dataset with a colmn \"file path\" wich contiains the paths listed in both t1w_file_paths and t2w_file_paths\n",
    "path_data = pd.DataFrame({\"image_path\" : t1w_file_paths})\n",
    "\n",
    "train_data, val_data = train_test_split(path_data, test_size=0.2, random_state=0)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 213\n"
     ]
    }
   ],
   "source": [
    "# Define a custom dataset class\n",
    "class Dataset_3D(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        # load image\n",
    "        self.data={}\n",
    "        self.data['paths'] = paths\n",
    "        self.transform = transform\n",
    "        self.length = len(paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paths = self.data['paths'][index]\n",
    "        if self.transform:\n",
    "            image = self.transform(paths)\n",
    "            # add a dimension to the image, for exemple [1, 256, 256, 256] -> [1, 1, 256, 256, 256]\n",
    "        # convert label list to tensor with shape [1,2]\n",
    "        print(image.shape)\n",
    "        return image\n",
    "    \n",
    "# use monai to define the transforms for data augmentation\n",
    "# perform the following transformations : rotation (random between +3° and -3°), flipping (random between 0°,  90 °, 180° and 270°), cropping (Random size, random place) and shifting (random shift)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True, ensure_channel_first=True),\n",
    "        RandRotate90(prob=0.5),\n",
    "        RandFlip(prob=0.5),\n",
    "        RandShiftIntensity(offsets=0.1, prob=0.5),\n",
    "        RandRotate(range_x=3, range_y=3, range_z=3, prob=0.2),\n",
    "        Resize((16, 16, 16), mode='trilinear', align_corners=True),\n",
    "        ToTensor(),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True, ensure_channel_first=True),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the custom datasets\n",
    "train_dataset = Dataset_3D(\n",
    "    paths=train_data['image_path'],\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "val_dataset = Dataset_3D(\n",
    "    paths=val_data['image_path'],\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "# Create the custom dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# define model and diffusion\n",
    "timesteps = 500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNetModel(\n",
    "    in_channels=1,\n",
    "    model_channels=96,\n",
    "    out_channels=1,\n",
    "    channel_mult=(1, 2, 2),\n",
    "    attention_resolutions=[]\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=timesteps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "torch.Size([1, 16, 16, 16])\n",
      "Epoch 1/1, Loss: 1.0311059951782227\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "# Fill here\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, timesteps, (x.shape[0],), device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = gaussian_diffusion.train_losses(model, x, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "#save model\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [96, 1, 3, 3, 3], expected input[1, 8, 3, 16, 16] to have 1 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## generate a 3D image from the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sample new images\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_diffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(sample, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 448\u001b[0m, in \u001b[0;36mGaussianDiffusion.sample\u001b[1;34m(self, model, image_size, batch_size, channels)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, image_size, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# sample new images\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 439\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[1;34m(self, model, shape)\u001b[0m\n\u001b[0;32m    437\u001b[0m x_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 439\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m     imgs\u001b[38;5;241m.\u001b[39mappend(x_t)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 418\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[1;34m(self, model, x_t, t, clip_denoised)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, x_t, t, clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# denoise_step: sample x_{t-1} from x_t and pred_noise\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;66;03m# predict mean and variance\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     model_mean, _, model_log_variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_t)\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# no noise when t == 0\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 397\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[1;34m(self, model, x_t, t, clip_denoised)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_mean_variance\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, x_t, t, clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# compute predicted mean and variance of p(x_{t-1} | x_t)\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# predict noise using model\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     pred_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# get the predicted x_0: different from the algorithm2 in the paper\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# model_mean, posterior_variance, posterior_log_variance\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# re-use q_posterior_mean_variance and predict_start_from_noise\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_start_from_noise(x_t, t, pred_noise)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 260\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[1;34m(self, x, timesteps)\u001b[0m\n\u001b[0;32m    258\u001b[0m h \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[1;32m--> 260\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# middle stage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     44\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, t)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bapti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [96, 1, 3, 3, 3], expected input[1, 8, 3, 16, 16] to have 1 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "## generate a 3D image from the model\n",
    "\n",
    "# sample new images\n",
    "sample = gaussian_diffusion.sample(model, 16)\n",
    "sample = torch.cat(sample, dim=0)\n",
    "sample = sample.cpu().numpy()\n",
    "\n",
    "# plot the generated image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(sample[0, 0, :, :, 8], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Create a dataset\n",
    "x = torch.randn(100, 3)  # 100 samples, each with 3 features\n",
    "y = torch.randn(100, 1)  # 100 labels\n",
    "dataset = TensorDataset(x)\n",
    "\n",
    "# Create a data loader\n",
    "loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Iterate over the data loader\n",
    "for batch_x in loader:\n",
    "    # batch_x and batch_y are tensors with 32 samples each\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
